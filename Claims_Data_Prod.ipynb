{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c2bd184-a541-44fa-857c-039f15d1305e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "006851fe-135d-44a2-a4ae-733f0502e14b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.ksrdatadlsa.dfs.core.windows.net\",\n",
    "    dbutils.secrets.get(scope=\"optumscope1\", key=\"optumkeysstore\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82eca4b1-e2d4-4b2f-9681-f4c6f07e1d5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Claims = spark.read.csv(\"abfss://optumdata@ksrdatadlsa.dfs.core.windows.net/RawData/Claims.csv\",header=True, inferSchema=True,escape='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c3f24ba-cf5b-4004-8c07-3b21fe7de539",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "Claims = Claims.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d49199-5838-4bd1-9bd1-78d6c3b4de89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(70, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to verify Rows count and Cloumn Count\n",
    "Claims.count(),len(Claims.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca51b602-72cd-4346-a4c4-f355503303d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------------+------+-----------------+----------+------------+----------+\n|claim_id|patient_id|disease_name|SUB_ID|Claim_Or_Rejected|claim_type|claim_amount|claim_date|\n+--------+----------+------------+------+-----------------+----------+------------+----------+\n|       0|         0|           0|     0|                0|         0|           0|         0|\n+--------+----------+------------+------+-----------------+----------+------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# To find null values\n",
    "from pyspark.sql.functions import count, col, isnan, when\n",
    "Claims.select([count(when(col(c).isNull(), c)).alias(c) for c in Claims.columns]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97360550-b6f2-4782-9c57-18a43e3a2290",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Count of claim_id column: 70\nNull values Count of claim_id column: 0\n\nclaim_id Column Start With Number: 70\nclaim_id Column Start With String: 0\nclaim_id Column Start With Spl.Cha: 0\nclaim_id Column contains string values: 0\nclaim_id Column contains number values: 70\nclaim_id Column contains Spl.Char values: 0\n\nDistinct Count of patient_id column: 70\nNull values Count of patient_id column: 0\n\npatient_id Column Start With Number: 70\npatient_id Column Start With String: 0\npatient_id Column Start With Spl.Cha: 0\npatient_id Column contains string values: 0\npatient_id Column contains number values: 70\npatient_id Column contains Spl.Char values: 0\n\nDistinct Count of disease_name column: 41\nNull values Count of disease_name column: 0\n\ndisease_name Column Start With Number: 0\ndisease_name Column Start With String: 70\ndisease_name Column Start With Spl.Cha: 0\ndisease_name Column contains string values: 70\ndisease_name Column contains number values: 0\ndisease_name Column contains Spl.Char values: 27\n\nDistinct Count of SUB_ID column: 70\nNull values Count of SUB_ID column: 0\n\nSUB_ID Column Start With Number: 0\nSUB_ID Column Start With String: 70\nSUB_ID Column Start With Spl.Cha: 0\nSUB_ID Column contains string values: 70\nSUB_ID Column contains number values: 70\nSUB_ID Column contains Spl.Char values: 0\n\nDistinct Count of Claim_Or_Rejected column: 3\nNull values Count of Claim_Or_Rejected column: 0\n\nClaim_Or_Rejected Column Start With Number: 0\nClaim_Or_Rejected Column Start With String: 70\nClaim_Or_Rejected Column Start With Spl.Cha: 0\nClaim_Or_Rejected Column contains string values: 70\nClaim_Or_Rejected Column contains number values: 0\nClaim_Or_Rejected Column contains Spl.Char values: 0\n\nDistinct Count of claim_type column: 3\nNull values Count of claim_type column: 0\n\nclaim_type Column Start With Number: 0\nclaim_type Column Start With String: 70\nclaim_type Column Start With Spl.Cha: 0\nclaim_type Column contains string values: 70\nclaim_type Column contains number values: 0\nclaim_type Column contains Spl.Char values: 70\n\nDistinct Count of claim_amount column: 70\nNull values Count of claim_amount column: 0\n\nclaim_amount Column Start With Number: 70\nclaim_amount Column Start With String: 0\nclaim_amount Column Start With Spl.Cha: 0\nclaim_amount Column contains string values: 0\nclaim_amount Column contains number values: 70\nclaim_amount Column contains Spl.Char values: 0\n\nDistinct Count of claim_date column: 70\nNull values Count of claim_date column: 0\n\nclaim_date Column Start With Number: 70\nclaim_date Column Start With String: 0\nclaim_date Column Start With Spl.Cha: 0\nclaim_date Column contains string values: 0\nclaim_date Column contains number values: 70\nclaim_date Column contains Spl.Char values: 70\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, asc\n",
    "\n",
    "def analyze_columns(df):\n",
    "    columns = df.columns\n",
    "    for column in columns:\n",
    "        distinct_count = df.select(column).distinct().count()\n",
    "        null_count = df.filter(col(column).isNull()).count()\n",
    "        numeric_count = df.filter(col(column).rlike(\"^[0-9]\")).count()\n",
    "        text_count = df.filter(col(column).rlike(\"^[A-Za-z]\")).count()\n",
    "        special_char_count = df.filter(~col(column).rlike(\"^[A-Za-z0-9]\")).count()\n",
    "        contains_string = df.filter(col(column).rlike(\"[A-Za-z]\")).count()\n",
    "        contains_number = df.filter(col(column).rlike(\"[0-9]\")).count()\n",
    "        contains_spl_char = df.filter(col(column).rlike(\"[^A-Za-z0-9]\")).count()\n",
    "\n",
    "        print(\"Distinct Count of {} column: {}\".format(column, distinct_count))\n",
    "        print(\"Null values Count of {} column: {}\\n\".format(column, null_count))\n",
    "        print(\"{} Column Start With Number: {}\".format(column, numeric_count))\n",
    "        print(\"{} Column Start With String: {}\".format(column, text_count))\n",
    "        print(\"{} Column Start With Spl.Cha: {}\".format(column, special_char_count))\n",
    "        print(\"{} Column contains string values: {}\".format(column, contains_string))\n",
    "        print(\"{} Column contains number values: {}\".format(column, contains_number))\n",
    "        print(\"{} Column contains Spl.Char values: {}\\n\".format(column, contains_spl_char))\n",
    "        \n",
    "# Example usage:\n",
    "analyze_columns(Claims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c261da3-2cbc-4ef3-ab5a-a88069379a45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|Claim_Or_Rejected|\n+-----------------+\n|                Y|\n|                N|\n|              NaN|\n+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "Claims.select(\"Claim_Or_Rejected\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc1c8d59-37f1-4981-b558-1e86f3e13197",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Claims = Claims.na.replace([\"NaN\"],\"N\", subset=[\"Claim_Or_Rejected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80314846-f077-4639-84e1-6ab7418b5d35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|Claim_Or_Rejected|\n+-----------------+\n|                Y|\n|                N|\n+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "Claims.select(\"Claim_Or_Rejected\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d2cd5e7-51cb-4e1c-aa5c-b9cae808a0a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n|      claim_type|\n+----------------+\n| claims of value|\n|  claims of fact|\n|claims of policy|\n+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "Claims.select(\"claim_type\").distinct().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f17a5a1-2666-476d-9c1d-9878de94aca6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the output container path\n",
    "output_container_path = \"abfss://optumdata@ksrdatadlsa.dfs.core.windows.net/StatgingData\"\n",
    "\n",
    "# Write the DataFrame to the output container path\n",
    "Claims.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").format(\"com.databricks.spark.csv\").save(output_container_path)\n",
    "\n",
    "# List files in the output container path\n",
    "files = dbutils.fs.ls(output_container_path)\n",
    "\n",
    "# Filter out the output file\n",
    "output_file = [x for x in files if x.name.startswith(\"part-\")]\n",
    "\n",
    "# Move the output file to a specific location\n",
    "dbutils.fs.mv(output_file[0].path, \"%s/stg_Claims.csv\" % output_container_path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Claims_Data_Prod",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
