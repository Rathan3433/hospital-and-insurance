{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d33f5cd-8bb5-4f05-9e9c-e4410037b807",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e1f4336-b661-4626-ae83-0448516596ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.ksrdatadlsa.dfs.core.windows.net\",\n",
    "    dbutils.secrets.get(scope=\"optumscope1\", key=\"optumkeysstore\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3dcaab6-8359-458f-8c9a-3051ef05cc2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Subscriber = spark.read.csv(\"abfss://optumdata@ksrdatadlsa.dfs.core.windows.net/RawData/subscriber.csv\",header=True, inferSchema=True,escape='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81ba8e3d-ee87-4755-af91-5d2505793603",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "Subscriber = Subscriber.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "297e73b0-425d-4644-8e79-6c89fd5912b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# to find Rows count and Cloumn Count\n",
    "Subscriber.count(),len(Subscriber.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57ea9996-ed33-4f03-9083-45e97fda92f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, asc\n",
    "\n",
    "def analyze_columns(df):\n",
    "    columns = df.columns\n",
    "    for column in columns:\n",
    "        distinct_count = df.select(column).distinct().count()\n",
    "        numeric_count = df.filter(col(column).rlike(\"^[0-9]\")).count()\n",
    "        text_count = df.filter(col(column).rlike(\"^[A-Za-z]\")).count()\n",
    "        special_char_count = df.filter(~col(column).rlike(\"^[A-Za-z0-9]\")).count()\n",
    "        contains_number = df.filter(col(column).rlike(\"[0-9]\")).count()\n",
    "        contains_string = df.filter(col(column).rlike(\"[A-Za-z]\")).count()\n",
    "        contains_spl_char = df.filter(col(column).rlike(\"[^A-Za-z0-9]\")).count()\n",
    "\n",
    "        print(\"Distinct Count of {} column: {}\".format(column, distinct_count))\n",
    "        print(\"{} Column Start With Number: {}\".format(column, numeric_count))\n",
    "        print(\"{} Column Start With String: {}\".format(column, text_count))\n",
    "        print(\"{} Column Start With Spl.Cha: {}\".format(column, special_char_count))\n",
    "         print(\"{} Column contains number values: {}\".format(column, contains_number))\n",
    "        print(\"{} Column contains string values: {}\".format(column, contains_string))\n",
    "        print(\"{} Column contains Spl.Char values: {}\\n\".format(column, contains_spl_char))\n",
    "        \n",
    "# Example usage:\n",
    "analyze_columns(Subscriber)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f41e75ce-1352-4dc7-ae4b-6f000c81456a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Subscriber = Subscriber.fillna(\"Guest\", subset=[\"first_name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07659a6-3e1a-4876-81bd-3d6ea55d1a82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Subscriber.filter(col(\"first_name\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "220f85a8-f2c4-4a5d-88a8-97f03a833b90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Subscriber.filter(col(\"first_name\").isNaN()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7470ff4b-d702-4037-a519-21319e430f4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Null Count:  3\n Null Count:  0\n+----------+----------+---------+-----------+----------+------+-----+-------+----------------+--------+---------+--------+----------+----------+\n|    sub_id|first_name|last_name|     Street|Birth_date|Gender|Phone|Country|            City|Zip Code|Subgrp_id|Elig_ind|  eff_date| term_date|\n+----------+----------+---------+-----------+----------+------+-----+-------+----------------+--------+---------+--------+----------+----------+\n| SUBID1036|   Upasana|   Thakur|  Vasa Ganj|1927-10-03|Female| NULL|  India|          Ratlam|  326733|     S108|       N|1947-10-03|1951-02-20|\n| SUBID1041|          |   Rajput| Sinha Path|1930-11-25|Female| NULL|  India|        Agartala|  303503|     S110|       Y|1950-11-25|1957-09-14|\n|SUBID10095|   Ekaaksh|      Rai|Bansal Ganj|1933-12-02|Others| NULL|  India|Pimpri-Chinchwad|  158186|     S107|       N|1953-12-02|1960-07-29|\n+----------+----------+---------+-----------+----------+------+-----+-------+----------------+--------+---------+--------+----------+----------+\n\nNone\n"
     ]
    }
   ],
   "source": [
    "print(\" Null Count: \" ,Subscriber.filter(col(\"Phone\").isNull()).count())\n",
    "print(\" Null Count: \" ,Subscriber.filter(col(\"Phone\").isNaN()).count())\n",
    "print(Subscriber.filter(col(\"Phone\").isNull()).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6d88254-d6b9-40f4-bdf3-55d8f18f2784",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Subscriber = Subscriber.fillna(\"NA\", subset=[\"Phone\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ad56bcf-787f-470d-9943-fd276e25a2a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To find null values\n",
    "from pyspark.sql.functions import count, col, isnan, when\n",
    "Subscriber.select([count(when(col(c).isNull(), c)).alias(c) for c in Subscriber.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e99e74fd-1345-4f4a-a1ec-69539ae34bfc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "Subscriber.filter(length(col(\"Zip Code\")) == 6).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec0388b2-675c-413d-9d45-55195dd27807",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "Subscriber.filter(length(col(\"Zip Code\")) < 6).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "120d0fd6-a767-4065-9143-64a90d88bf1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "Subscriber.filter(length(col(\"Zip Code\")) > 6).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f20f9aa5-43a7-40aa-825d-48194a59dcb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Define replacement conditions using when function\n",
    "conditions = [\n",
    "    (col(\"city\") == \"Tiruvottiyur\", 600019),\n",
    "    (col(\"city\") == \"Bihar Sharif\", 803118),\n",
    "    (col(\"city\") == \"Hapur\", 245101),\n",
    "    (col(\"city\") == \"Kochi\", 682001),\n",
    "    (col(\"city\") == \"Hajipur\", 844101),\n",
    "    (col(\"city\") == \"Ghaziabad\", 201003),\n",
    "    (col(\"city\") == \"Navi Mumbai\", 400001)\n",
    "]\n",
    "\n",
    "# Apply the conditions using otherwise function\n",
    "Subscriber = Subscriber.withColumn(\"Zip Code\", when(conditions[0][0], conditions[0][1])\n",
    "                                                  .when(conditions[1][0], conditions[1][1])\n",
    "                                                  .when(conditions[2][0], conditions[2][1])\n",
    "                                                  .when(conditions[3][0], conditions[3][1])\n",
    "                                                  .when(conditions[4][0], conditions[4][1])\n",
    "                                                  .when(conditions[5][0], conditions[5][1])\n",
    "                                                  .when(conditions[6][0], conditions[6][1])\n",
    "                                                  .otherwise(col(\"Zip Code\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d07fd348-716b-44ef-b4f0-4a9cc9536f8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To find null values\n",
    "from pyspark.sql.functions import count, col, isnan, when\n",
    "Subscriber.select([count(when(col(c).isNull(), c)).alias(c) for c in Subscriber.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7632d653-dfa8-41c9-808d-dff69cedac4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "Subscriber = Subscriber.withColumn(\"Subgrp_id\", \n",
    "                    when(Subscriber[\"SUB_ID\"] == \"SUBID10022\", \"S110\")\n",
    "                   .when(Subscriber[\"SUB_ID\"] == \"SUBID10049\", \"S107\")\n",
    "                   .otherwise(Subscriber[\"Subgrp_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb40cb2d-ad08-449a-93b6-febafe5d0365",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+------+----------+------+-----+-------+----+--------+---------+--------+--------+---------+\n|sub_id|first_name|last_name|Street|Birth_date|Gender|Phone|Country|City|Zip Code|Subgrp_id|Elig_ind|eff_date|term_date|\n+------+----------+---------+------+----------+------+-----+-------+----+--------+---------+--------+--------+---------+\n+------+----------+---------+------+----------+------+-----+-------+----+--------+---------+--------+--------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "Subscriber.filter(col(\"Subgrp_id\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff569791-ad2a-475e-970b-04a8dcd88d35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Subscriber.filter(col(\"Elig_ind\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55984fba-1137-4514-9d2f-9cd243e8e0cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Subscriber = Subscriber_TB.fillna(\"N\", subset=\"Elig_ind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac8a808c-3715-49ca-89e7-cc6be8204d3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Subscriber.filter(col(\"Elig_ind\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef48d948-17aa-447f-a39c-7e668a7cf7b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, current_date, to_date\n",
    "# Convert the 'patient_birth_date' column to a DateType\n",
    "Subscriber = Subscriber.withColumn(\"Birth_date\", to_date(\"Birth_date\"))\n",
    "# Calculate the age based on the birth date\n",
    "Subscriber = Subscriber.withColumn(\"age\", year(current_date()) - year(\"Birth_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2bed78e-2c71-4b65-a9f8-ca3b1068456d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Subscriber = Subscriber.drop(col(\"Birth_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c146eaf1-c77d-41bc-82a4-6f063887ba90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Subscriber.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83a4be14-dfc2-4ad6-9d77-55d7acf3a1d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the output container path\n",
    "output_container_path = \"abfss://optumdata@ksrdatadlsa.dfs.core.windows.net/StatgingData\"\n",
    "\n",
    "# Write the DataFrame to the output container path\n",
    "Subscriber.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").format(\"com.databricks.spark.csv\").save(output_container_path)\n",
    "\n",
    "# List files in the output container path\n",
    "files = dbutils.fs.ls(output_container_path)\n",
    "\n",
    "# Filter out the output file\n",
    "output_file = [x for x in files if x.name.startswith(\"part-\")]\n",
    "\n",
    "# Move the output file to a specific location\n",
    "dbutils.fs.mv(output_file[0].path, \"%s/stg_Subscriber.csv\" % output_container_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Subscriber_Data_Prod",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
