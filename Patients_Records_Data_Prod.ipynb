{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c2bd184-a541-44fa-857c-039f15d1305e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "006851fe-135d-44a2-a4ae-733f0502e14b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.ksrdatadlsa.dfs.core.windows.net\",\n",
    "    dbutils.secrets.get(scope=\"optumscope1\", key=\"optumkeysstore\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82eca4b1-e2d4-4b2f-9681-f4c6f07e1d5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Patient_Records = spark.read.csv(\"abfss://optumdata@ksrdatadlsa.dfs.core.windows.net/RawData/Patient_records.csv\",header=True, inferSchema=True,escape='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c3f24ba-cf5b-4004-8c07-3b21fe7de539",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "Patient_Records = Patient_Records.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d49199-5838-4bd1-9bd1-78d6c3b4de89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(70, 8)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to verify Rows count and Cloumn Count\n",
    "Patient_Records.count(),len(Patient_Records.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca51b602-72cd-4346-a4c4-f355503303d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------+------------------+-------------+------------+----+-----------+\n|Patient_id|Patient_name|patient_gender|patient_birth_date|patient_phone|disease_name|city|hospital_id|\n+----------+------------+--------------+------------------+-------------+------------+----+-----------+\n|         0|          17|             0|                 0|            2|           0|   0|          0|\n+----------+------------+--------------+------------------+-------------+------------+----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# To find null values\n",
    "from pyspark.sql.functions import count, col, isnan, when\n",
    "Patient_Records.select([count(when(col(c).isNull(), c)).alias(c) for c in Patient_Records.columns]).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "243490ba-17c2-46ea-b01b-e9dcd1cb884f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Patient_Records = Patient_Records.fillna(\"Guest\",subset=[\"Patient_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e7ebcd3-50c2-413e-8842-4e5a9762ec2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop phone number column\n",
    "Patient_Records = Patient_Records.fillna(\"NA\", subset=\"patient_phone\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8ed42a5-e1ac-45b6-b7ee-dc1f349cf325",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, current_date, to_date\n",
    "# Convert the 'patient_birth_date' column to a DateType\n",
    "Patient_Records = Patient_Records.withColumn(\"patient_birth_date\", to_date(\"patient_birth_date\"))\n",
    "# Calculate the age based on the birth date\n",
    "Patient_Records = Patient_Records.withColumn(\"age\", year(current_date()) - year(\"patient_birth_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b73d20-b00f-41d7-8711-b3005380dd63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Patient_Records = Patient_Records.drop(col(\"patient_birth_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97360550-b6f2-4782-9c57-18a43e3a2290",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Count of Patient_id column: 70\nNull values Count of Patient_id column: 0\n\nPatient_id Column Start With Number: 70\nPatient_id Column Start With String: 0\nPatient_id Column Start With Spl.Cha: 0\nPatient_id Column contains string values: 0\nPatient_id Column contains number values: 70\nPatient_id Column contains Spl.Char values: 0\n\nDistinct Count of Patient_name column: 53\nNull values Count of Patient_name column: 0\n\nPatient_name Column Start With Number: 0\nPatient_name Column Start With String: 70\nPatient_name Column Start With Spl.Cha: 0\nPatient_name Column contains string values: 70\nPatient_name Column contains number values: 0\nPatient_name Column contains Spl.Char values: 0\n\nDistinct Count of patient_gender column: 2\nNull values Count of patient_gender column: 0\n\npatient_gender Column Start With Number: 0\npatient_gender Column Start With String: 70\npatient_gender Column Start With Spl.Cha: 0\npatient_gender Column contains string values: 70\npatient_gender Column contains number values: 0\npatient_gender Column contains Spl.Char values: 0\n\nDistinct Count of patient_phone column: 69\nNull values Count of patient_phone column: 0\n\npatient_phone Column Start With Number: 0\npatient_phone Column Start With String: 2\npatient_phone Column Start With Spl.Cha: 68\npatient_phone Column contains string values: 2\npatient_phone Column contains number values: 68\npatient_phone Column contains Spl.Char values: 68\n\nDistinct Count of disease_name column: 41\nNull values Count of disease_name column: 0\n\ndisease_name Column Start With Number: 0\ndisease_name Column Start With String: 70\ndisease_name Column Start With Spl.Cha: 0\ndisease_name Column contains string values: 70\ndisease_name Column contains number values: 0\ndisease_name Column contains Spl.Char values: 27\n\nDistinct Count of city column: 62\nNull values Count of city column: 0\n\ncity Column Start With Number: 0\ncity Column Start With String: 70\ncity Column Start With Spl.Cha: 0\ncity Column contains string values: 70\ncity Column contains number values: 0\ncity Column contains Spl.Char values: 5\n\nDistinct Count of hospital_id column: 20\nNull values Count of hospital_id column: 0\n\nhospital_id Column Start With Number: 0\nhospital_id Column Start With String: 70\nhospital_id Column Start With Spl.Cha: 0\nhospital_id Column contains string values: 70\nhospital_id Column contains number values: 70\nhospital_id Column contains Spl.Char values: 0\n\nDistinct Count of age column: 45\nNull values Count of age column: 0\n\nage Column Start With Number: 70\nage Column Start With String: 0\nage Column Start With Spl.Cha: 0\nage Column contains string values: 0\nage Column contains number values: 70\nage Column contains Spl.Char values: 0\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, asc\n",
    "\n",
    "def analyze_columns(df):\n",
    "    columns = df.columns\n",
    "    for column in columns:\n",
    "        distinct_count = df.select(column).distinct().count()\n",
    "        null_count = df.filter(col(column).isNull()).count()\n",
    "        numeric_count = df.filter(col(column).rlike(\"^[0-9]\")).count()\n",
    "        text_count = df.filter(col(column).rlike(\"^[A-Za-z]\")).count()\n",
    "        special_char_count = df.filter(~col(column).rlike(\"^[A-Za-z0-9]\")).count()\n",
    "        contains_string = df.filter(col(column).rlike(\"[A-Za-z]\")).count()\n",
    "        contains_number = df.filter(col(column).rlike(\"[0-9]\")).count()\n",
    "        contains_spl_char = df.filter(col(column).rlike(\"[^A-Za-z0-9]\")).count()\n",
    "\n",
    "        print(\"Distinct Count of {} column: {}\".format(column, distinct_count))\n",
    "        print(\"Null values Count of {} column: {}\\n\".format(column, null_count))\n",
    "        print(\"{} Column Start With Number: {}\".format(column, numeric_count))\n",
    "        print(\"{} Column Start With String: {}\".format(column, text_count))\n",
    "        print(\"{} Column Start With Spl.Cha: {}\".format(column, special_char_count))\n",
    "        print(\"{} Column contains string values: {}\".format(column, contains_string))\n",
    "        print(\"{} Column contains number values: {}\".format(column, contains_number))\n",
    "        print(\"{} Column contains Spl.Char values: {}\\n\".format(column, contains_spl_char))\n",
    "        \n",
    "# Example usage:\n",
    "analyze_columns(Patient_Records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f17a5a1-2666-476d-9c1d-9878de94aca6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the output container path\n",
    "output_container_path = \"abfss://optumdata@ksrdatadlsa.dfs.core.windows.net/StatgingData\"\n",
    "\n",
    "# Write the DataFrame to the output container path\n",
    "Patient_Records.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").format(\"com.databricks.spark.csv\").save(output_container_path)\n",
    "\n",
    "# List files in the output container path\n",
    "files = dbutils.fs.ls(output_container_path)\n",
    "\n",
    "# Filter out the output file\n",
    "output_file = [x for x in files if x.name.startswith(\"part-\")]\n",
    "\n",
    "# Move the output file to a specific location\n",
    "dbutils.fs.mv(output_file[0].path, \"%s/stg_Patient_Records.csv\" % output_container_path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Patients_Records_Data_Prod",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
