{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c2bd184-a541-44fa-857c-039f15d1305e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "006851fe-135d-44a2-a4ae-733f0502e14b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.ksrdatadlsa.dfs.core.windows.net\",\n",
    "    dbutils.secrets.get(scope=\"optumscope1\", key=\"optumkeysstore\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82eca4b1-e2d4-4b2f-9681-f4c6f07e1d5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Group = spark.read.csv(\"abfss://optumdata@ksrdatadlsa.dfs.core.windows.net/RawData/group.csv\",header=True, inferSchema=True,escape='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c3f24ba-cf5b-4004-8c07-3b21fe7de539",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "Group = Group.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d49199-5838-4bd1-9bd1-78d6c3b4de89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(58, 7)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to verify Rows count and Cloumn Count\n",
    "Group.count(),len(Group.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca51b602-72cd-4346-a4c4-f355503303d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------+------+--------+--------+----+\n|country|premium_written|zip_code|grp_id|grp_name|grp_type|city|\n+-------+---------------+--------+------+--------+--------+----+\n|      0|              0|       0|     0|       0|       0|   0|\n+-------+---------------+--------+------+--------+--------+----+\n\n"
     ]
    }
   ],
   "source": [
    "# To find null values\n",
    "Group.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in Group.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0408fb10-befa-414b-859d-f7ba1fbd65fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# to replace New Delhi with Delhi in city column\n",
    "Group = Group.replace([\"New Delhi\"], \"Delhi\",\"city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97360550-b6f2-4782-9c57-18a43e3a2290",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Count of country column: 1\nNull values Count of country column: 0\n\ncountry Column Start With Number: 0\ncountry Column Start With String: 58\ncountry Column Start With Spl.Cha: 0\ncountry Column contains string values: 58\ncountry Column contains number values: 0\ncountry Column contains Spl.Char values: 0\n\nDistinct Count of premium_written column: 39\nNull values Count of premium_written column: 0\n\npremium_written Column Start With Number: 58\npremium_written Column Start With String: 0\npremium_written Column Start With Spl.Cha: 0\npremium_written Column contains string values: 0\npremium_written Column contains number values: 58\npremium_written Column contains Spl.Char values: 0\n\nDistinct Count of zip_code column: 36\nNull values Count of zip_code column: 0\n\nzip_code Column Start With Number: 58\nzip_code Column Start With String: 0\nzip_code Column Start With Spl.Cha: 0\nzip_code Column contains string values: 0\nzip_code Column contains number values: 58\nzip_code Column contains Spl.Char values: 0\n\nDistinct Count of grp_id column: 58\nNull values Count of grp_id column: 0\n\ngrp_id Column Start With Number: 0\ngrp_id Column Start With String: 58\ngrp_id Column Start With Spl.Cha: 0\ngrp_id Column contains string values: 58\ngrp_id Column contains number values: 58\ngrp_id Column contains Spl.Char values: 0\n\nDistinct Count of grp_name column: 58\nNull values Count of grp_name column: 0\n\ngrp_name Column Start With Number: 0\ngrp_name Column Start With String: 58\ngrp_name Column Start With Spl.Cha: 0\ngrp_name Column contains string values: 58\ngrp_name Column contains number values: 0\ngrp_name Column contains Spl.Char values: 58\n\nDistinct Count of grp_type column: 2\nNull values Count of grp_type column: 0\n\ngrp_type Column Start With Number: 0\ngrp_type Column Start With String: 58\ngrp_type Column Start With Spl.Cha: 0\ngrp_type Column contains string values: 58\ngrp_type Column contains number values: 0\ngrp_type Column contains Spl.Char values: 7\n\nDistinct Count of city column: 11\nNull values Count of city column: 0\n\ncity Column Start With Number: 0\ncity Column Start With String: 58\ncity Column Start With Spl.Cha: 0\ncity Column contains string values: 58\ncity Column contains number values: 0\ncity Column contains Spl.Char values: 0\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, asc\n",
    "\n",
    "def analyze_columns(df):\n",
    "    columns = df.columns\n",
    "    for column in columns:\n",
    "        distinct_count = df.select(column).distinct().count()\n",
    "        null_count = df.filter(col(column).isNull()).count()\n",
    "        numeric_count = df.filter(col(column).rlike(\"^[0-9]\")).count()\n",
    "        text_count = df.filter(col(column).rlike(\"^[A-Za-z]\")).count()\n",
    "        special_char_count = df.filter(~col(column).rlike(\"^[A-Za-z0-9]\")).count()\n",
    "        contains_string = df.filter(col(column).rlike(\"[A-Za-z]\")).count()\n",
    "        contains_number = df.filter(col(column).rlike(\"[0-9]\")).count()\n",
    "        contains_spl_char = df.filter(col(column).rlike(\"[^A-Za-z0-9]\")).count()\n",
    "\n",
    "        print(\"Distinct Count of {} column: {}\".format(column, distinct_count))\n",
    "        print(\"Null values Count of {} column: {}\\n\".format(column, null_count))\n",
    "        print(\"{} Column Start With Number: {}\".format(column, numeric_count))\n",
    "        print(\"{} Column Start With String: {}\".format(column, text_count))\n",
    "        print(\"{} Column Start With Spl.Cha: {}\".format(column, special_char_count))\n",
    "        print(\"{} Column contains string values: {}\".format(column, contains_string))\n",
    "        print(\"{} Column contains number values: {}\".format(column, contains_number))\n",
    "        print(\"{} Column contains Spl.Char values: {}\\n\".format(column, contains_spl_char))\n",
    "        \n",
    "# Example usage:\n",
    "analyze_columns(Group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58741ee3-6d77-4146-8ab1-e948ef21f825",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Group.filter(length(col(\"zip_code\")) < 6).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b24a745-4645-4991-b7b6-296c3e56b235",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Group.filter(length(col(\"zip_code\")) == 6).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "436f0dde-394b-41c6-aebc-7de18a480726",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Group.filter(col(\"city\") == \"Delhi\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f17a5a1-2666-476d-9c1d-9878de94aca6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the output container path\n",
    "output_container_path = \"abfss://optumdata@ksrdatadlsa.dfs.core.windows.net/StatgingData\"\n",
    "\n",
    "# Write the DataFrame to the output container path\n",
    "Group.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").format(\"com.databricks.spark.csv\").save(output_container_path)\n",
    "\n",
    "# List files in the output container path\n",
    "files = dbutils.fs.ls(output_container_path)\n",
    "\n",
    "# Filter out the output file\n",
    "output_file = [x for x in files if x.name.startswith(\"part-\")]\n",
    "\n",
    "# Move the output file to a specific location\n",
    "dbutils.fs.mv(output_file[0].path, \"%s/stg_Group.csv\" % output_container_path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Group_Data_Prod",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
