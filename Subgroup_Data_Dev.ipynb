{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c76dc13-e7d3-441b-a1d1-97c37d0b7a69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3405e79-03aa-46d8-b2d0-f27cf5275bd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.secrets.listScopes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e94ec7e1-906b-4e31-99a3-6fc1d7f8e20e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.secrets.list(scope = \"optumscope1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bbafab4-4b85-4c1a-ba12-10051892066e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.ksrdatadlsa.dfs.core.windows.net\",\n",
    "    dbutils.secrets.get(scope=\"optumscope1\", key=\"optumkeysstore\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57a12fa9-4d96-4bf5-8c79-0b058c1d6f10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"abfss://optumdata@ksrdatadlsa.dfs.core.windows.net/RawData\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3317b219-789c-4ad7-af6a-ae8b9cff468c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SubGroup = spark.read.csv(\"abfss://optumdata@ksrdatadlsa.dfs.core.windows.net/RawData/subgroup.csv\",header=True, inferSchema=True,escape='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e188cdc-09e5-4ab9-8875-42a265981f18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SubGroup.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a04d120e-1fa5-42a2-98cb-bb281d213693",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SubGroup.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3046f4e-81b2-47a6-84d9-24677c181f96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SubGroup.count(),len(SubGroup.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e4f5780-0c1b-4667-a4ab-b918a3b13409",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SubGroup = SubGroup.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7db7ed1e-13c6-4fc9-8639-a668f83c4e4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SubGroup.count(),len(SubGroup.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61eec294-f898-4e58-b5ba-050ec859fa33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To find null values\n",
    "from pyspark.sql.functions import count, col, isnan, when\n",
    "SubGroup.select([count(when(col(c).isNull(), c)).alias(c) for c in SubGroup.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89c7dcb7-1357-4ca6-b2a7-82c319be26f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, asc\n",
    "\n",
    "def analyze_columns(df):\n",
    "    columns = df.columns\n",
    "    for column in columns:\n",
    "        distinct_count = df.select(column).distinct().count() \n",
    "        numeric_count = df.filter(col(column).rlike(\"^[0-9]\")).count()\n",
    "        text_count = df.filter(col(column).rlike(\"^[A-Za-z]\")).count()\n",
    "        special_char_count = df.filter(~col(column).rlike(\"^[A-Za-z0-9]\")).count()\n",
    "        contains_number = df.filter(col(column).rlike(\"[0-9]\")).count()\n",
    "        contains_string = df.filter(col(column).rlike(\"[A-Za-z]\")).count()\n",
    "        contains_spl_char = df.filter(col(column).rlike(\"[^A-Za-z0-9]\")).count()\n",
    "\n",
    "        print(\"Distinct Count of {} column: {}\".format(column, distinct_count))\n",
    "        print(\"{} Column Start With Number: {}\".format(column, numeric_count))\n",
    "        print(\"{} Column Start With String: {}\".format(column, text_count))\n",
    "        print(\"{} Column Start With Spl.Cha: {}\".format(column, special_char_count))\n",
    "        print(\"{} Column contains number values: {}\".format(column, contains_number))\n",
    "        print(\"{} Column contains string values: {}\".format(column, contains_string))\n",
    "        print(\"{} Column contains Spl.Char values: {}\\n\".format(column, contains_spl_char))\n",
    "        \n",
    "# Example usage:\n",
    "analyze_columns(SubGroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3eec38b3-7e77-4bbb-890a-3745d84dafd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "# Split the \"subgrp_id\" column by delimiter (\",\") into an array\n",
    "SubGroup = SubGroup.withColumn(\"subgrp_id\", split(SubGroup[\"subgrp_id\"], \",\"))\n",
    "\n",
    "# Explode the array to create one row per element in the array\n",
    "SubGroup = SubGroup.withColumn(\"subgrp_id\", explode(SubGroup[\"subgrp_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "015a8a90-5fc4-4a53-bead-28c563128108",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show the DataFrame with the \"subgrp_id\" column split into separate cells\n",
    "SubGroup.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aafae0ce-128f-4840-bb51-5ab8c9b3ee73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the output container path\n",
    "output_container_path = \"abfss://optumdata@ksrdatadlsa.dfs.core.windows.net/StatgingData\"\n",
    "\n",
    "# Write the DataFrame to the output container path\n",
    "SubGroup.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").format(\"com.databricks.spark.csv\").save(output_container_path)\n",
    "\n",
    "# List files in the output container path\n",
    "files = dbutils.fs.ls(output_container_path)\n",
    "\n",
    "# Filter out the output file\n",
    "output_file = [x for x in files if x.name.startswith(\"part-\")]\n",
    "\n",
    "# Move the output file to a specific location\n",
    "dbutils.fs.mv(output_file[0].path, \"%s/stg_SubGroup.csv\" % output_container_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Subgroup_Data_Dev",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
